{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load df_wikitext_sentences from csv\n",
    "df_wikitext_sentences = pd.read_csv(\"../data/wikitext_sentences.csv\")\n",
    "# load df_controlled from csv\n",
    "df_controlled = pd.read_csv(\"../data/controlled_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wikitext_sentences.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_controlled.groupby(['Category', 'Inflection Label']).sample(n=2).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique lemmas in df_wikitext_sentences\n",
    "unique_lemmas_wikitext = df_wikitext_sentences['Lemma'].nunique()\n",
    "# how many unique lemmas in df_controlled\n",
    "unique_lemmas_controlled = df_controlled['Lemma'].nunique()\n",
    "print(f\"Unique lemmas in wikitext sentences: {unique_lemmas_wikitext}\")\n",
    "print(f\"Unique lemmas in controlled sentences: {unique_lemmas_controlled}\")\n",
    "\n",
    "# how many sentences in df_wikitext_sentences\n",
    "num_sentences_wikitext = df_wikitext_sentences.shape[0]\n",
    "# how many sentences in df_controlled\n",
    "num_sentences_controlled = df_controlled.shape[0]\n",
    "print(f\"Number of sentences in wikitext sentences: {num_sentences_wikitext}\")\n",
    "print(f\"Number of sentences in controlled sentences: {num_sentences_controlled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = \"ud_gum_dataset\"\n",
    "model_name = \"gpt2\"\n",
    "probe_dirs = [\n",
    "    f\"../output/probes/{dataset}_{model_name}_lexeme_dense\",\n",
    "    f\"../output/probes/{dataset}_{model_name}_multiclass_inflection_dense\", \n",
    "]\n",
    "\n",
    "def extract_probe_type(probe_dir):\n",
    "    \"\"\"Extracts the probe type from the directory name.\"\"\"\n",
    "    match = re.search(r\"\"+dataset+r\"_(.*?)_(dense|sparse)\", probe_dir)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}_{match.group(2)}\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Create a directory to save the figures\n",
    "output_dir = \"figures\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for probe_dir in probe_dirs:\n",
    "    probe_type = extract_probe_type(probe_dir)\n",
    "    csv_path = os.path.join(probe_dir, \"probe_results.csv\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Determine whether to plot inflection or lexeme accuracies\n",
    "        if \"inflection\" in probe_type:\n",
    "            task_col = \"Inflection\"\n",
    "            control_col = \"Inflection_Control\"\n",
    "        else:\n",
    "            task_col = \"Lexeme\"\n",
    "            control_col = \"Lexeme_Control\"\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))  # Create a new figure for each plot\n",
    "        ax.bar(df['Layer'], df[task_col], label=f\"{task_col} Accuracy\", alpha=0.7)\n",
    "        ax.bar(df['Layer'], df[control_col], label=f\"{control_col} Accuracy\", alpha=0.7)\n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.set_title(f\"{probe_type}: {task_col} vs. {control_col} Accuracy\")\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 1)  # Set y-axis limit to 0-1 for accuracy\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure to the output directory\n",
    "        output_path = os.path.join(output_dir, f\"{dataset}_{probe_type}.png\")\n",
    "        plt.savefig(output_path)\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))  # Create a new figure for each plot\n",
    "        ax.text(0.5, 0.5, f\"CSV Not Found for {probe_type}\", ha='center', va='center', color='red')\n",
    "        ax.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure to the output directory\n",
    "        output_path = os.path.join(output_dir, f\"{probe_type}_error.png\")\n",
    "        plt.savefig(output_path)\n",
    "        plt.close(fig)  # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the CSV file path as necessary\n",
    "csv_file = \"../data/wikitext_sentences.csv\"  # Change this to the path of your dataset CSV file\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"Dataset loaded with {len(df)} examples.\\n\")\n",
    "\n",
    "# --- 1. Dataset Balance Diagnostics ---\n",
    "\n",
    "# Check distribution of Lemma labels\n",
    "print(\"Lemma Distribution (count per class):\")\n",
    "lemma_counts = df[\"Lemma\"].value_counts()\n",
    "print(lemma_counts, \"\\n\")\n",
    "\n",
    "# Plot the lemma distribution as a bar chart (if there are many classes, you might want to limit the number displayed)\n",
    "plt.figure(figsize=(12, 6))\n",
    "lemma_counts.plot(kind=\"bar\")\n",
    "plt.title(\"Lemma Distribution\")\n",
    "plt.xlabel(\"Lemma\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks([])  # Remove x-axis labels\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check distribution of inflection labels (e.g., \"present\" vs. \"past\")\n",
    "print(\"Inflection Label Distribution (count per class):\")\n",
    "inflection_counts = df[\"Inflection Label\"].value_counts()\n",
    "print(inflection_counts, \"\\n\")\n",
    "\n",
    "# Plot the inflection label distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "inflection_counts.plot(kind=\"bar\", color='skyblue')\n",
    "plt.title(\"Inflection Label Distribution\")\n",
    "plt.xlabel(\"Inflection Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 2. Tokenization Diagnostics for Lemma ---\n",
    "\n",
    "# Load GPT-2 tokenizer (or change to the appropriate model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define a helper function to tokenize a given word\n",
    "def tokenize_word(word):\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to the Lemma column (ensure it is string type)\n",
    "df[\"Tokenized Lemma\"] = df[\"Lemma\"].astype(str).apply(tokenize_word)\n",
    "\n",
    "# Display a few examples of tokenized Lemmas\n",
    "print(\"Examples of Lemma Tokenization:\")\n",
    "print(df[[\"Lemma\", \"Tokenized Lemma\"]].head(10), \"\\n\")\n",
    "\n",
    "# Diagnose how many Lemma instances are tokenized into multiple tokens\n",
    "df[\"Num_Tokens\"] = df[\"Tokenized Lemma\"].apply(len)\n",
    "multi_token_count = (df[\"Num_Tokens\"] > 1).sum()\n",
    "print(f\"Number of lemma instances tokenized into multiple tokens: {multi_token_count}\")\n",
    "print(f\"Percentage of multi-token lemmas: {100 * multi_token_count/len(df):.2f}%\\n\")\n",
    "\n",
    "# Additionally, check the uniqueness across Lemma types:\n",
    "unique_lemmas = df[\"Lemma\"].dropna().unique()\n",
    "multi_token_unique = sum(1 for lex in unique_lemmas if len(tokenize_word(str(lex))) > 1)\n",
    "print(f\"Unique lemmas tokenized into multiple tokens: {multi_token_unique} out of {len(unique_lemmas)}\")\n",
    "\n",
    "# --- 3. Additional Diagnostics ---\n",
    "\n",
    "# You can include other diagnostics as needed, for instance:\n",
    "# - Checking for missing or NaN values in key columns\n",
    "for column in [\"Lemma\", \"Inflection Label\", \"Sentence\", \"Target Index\"]:\n",
    "    missing = df[column].isna().sum()\n",
    "    print(f\"Missing values in '{column}': {missing}\")\n",
    "\n",
    "# - Basic statistics: total number of examples, unique lemmas, etc.\n",
    "print(f\"\\nTotal number of examples: {len(df)}\")\n",
    "print(f\"Number of unique lemmas: {len(unique_lemmas)}\")\n",
    "print(f\"Number of unique inflection labels: {df['Inflection Label'].nunique()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu",
   "language": "python",
   "name": "cmu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
