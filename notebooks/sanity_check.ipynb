{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504b2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Configuration for all supported models\n",
    "MODEL_CONFIGS = {\n",
    "    \"gpt2\": {\n",
    "        \"model_name\": \"gpt2\",\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "    },\n",
    "    \"pythia1.4b\": {\n",
    "        \"model_name\": \"EleutherAI/pythia-1.4b-v0\",\n",
    "        \"tokenizer_name\": \"EleutherAI/pythia-1.4b-v0\",\n",
    "    },\n",
    "    \"gemma2b\": {\n",
    "        \"model_name\": \"google/gemma-2-2b\",\n",
    "        \"tokenizer_name\": \"google/gemma-2-2b\",\n",
    "    },\n",
    "    \"qwen2\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"tokenizer_name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    },\n",
    "    \"bert-base-uncased\": {\n",
    "        \"model_name\": \"bert-base-uncased\",\n",
    "        \"tokenizer_name\": \"bert-base-uncased\",\n",
    "    },\n",
    "    \"bert-large-uncased\": {\n",
    "        \"model_name\": \"bert-large-uncased\",\n",
    "        \"tokenizer_name\": \"bert-large-uncased\",\n",
    "    },\n",
    "    \"distilbert-base-uncased\": {\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"tokenizer_name\": \"distilbert-base-uncased\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def load_model_and_embeddings(model_key: str):\n",
    "    if model_key not in MODEL_CONFIGS:\n",
    "        valid = \", \".join(MODEL_CONFIGS.keys())\n",
    "        raise ValueError(f\"Unknown model '{model_key}'. Valid keys: {valid}\")\n",
    "    cfg = MODEL_CONFIGS[model_key]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
    "    model = AutoModel.from_pretrained(cfg[\"model_name\"])\n",
    "    emb_layer = model.get_input_embeddings()\n",
    "    return tokenizer, emb_layer.weight.data\n",
    "\n",
    "def get_embedding(tokenizer, embeddings, word: str):\n",
    "    tokens = tokenizer.tokenize(word, add_special_tokens=False)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    vecs = embeddings[ids]\n",
    "    return vecs.mean(dim=0)\n",
    "\n",
    "def find_closest(embeddings, query_vec, tokenizer, top_k=5):\n",
    "    emb_norm = F.normalize(embeddings, dim=1)\n",
    "    q_norm = F.normalize(query_vec.unsqueeze(0), dim=1)\n",
    "    sims = torch.mm(q_norm, emb_norm.t()).squeeze(0)\n",
    "    vals, idxs = torch.topk(sims, top_k + 20)\n",
    "    results = []\n",
    "    for score, idx in zip(vals.tolist(), idxs.tolist()):\n",
    "        tok = tokenizer.decode([idx]).strip()\n",
    "        if tok.isalpha():\n",
    "            results.append((tok, score))\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "def analogy_a_minus_b_plus_c(tokenizer, embeddings, a, b, c, top_k=5):\n",
    "    va = get_embedding(tokenizer, embeddings, a)\n",
    "    vb = get_embedding(tokenizer, embeddings, b)\n",
    "    vc = get_embedding(tokenizer, embeddings, c)\n",
    "    query = va - vb + vc\n",
    "    return find_closest(embeddings, query, tokenizer, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47f6fa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'qwen2': embedding dim = 1536, vocab size = 151936\n"
     ]
    }
   ],
   "source": [
    "model_key = \"qwen2\"\n",
    "\n",
    "tokenizer, embeddings = load_model_and_embeddings(model_key)\n",
    "dim = embeddings.shape[1]\n",
    "vocab_size = embeddings.shape[0]\n",
    "print(f\"Loaded '{model_key}': embedding dim = {dim}, vocab size = {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f5a7c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analogy (king - man + woman):\n",
      "  king         cosine_sim=0.6939\n",
      "  KING         cosine_sim=0.4254\n",
      "  woman        cosine_sim=0.4185\n",
      "  King         cosine_sim=0.3925\n",
      "  queen        cosine_sim=0.3881\n",
      "\n",
      "Analogy (man - king + queen):\n",
      "  man          cosine_sim=0.6176\n",
      "  MAN          cosine_sim=0.4735\n",
      "  queen        cosine_sim=0.4342\n",
      "  Man          cosine_sim=0.4330\n",
      "  man          cosine_sim=0.4305\n",
      "\n",
      "Analogy (walked - walk + jump):\n",
      "  jump         cosine_sim=0.7297\n",
      "  jump         cosine_sim=0.5977\n",
      "  Jump         cosine_sim=0.5395\n",
      "  Jump         cosine_sim=0.5100\n",
      "  jumping      cosine_sim=0.5046\n",
      "\n",
      "Analogy (go - went + run):\n",
      "  run          cosine_sim=0.6994\n",
      "  run          cosine_sim=0.5766\n",
      "  Run          cosine_sim=0.5133\n",
      "  Run          cosine_sim=0.4999\n",
      "  go           cosine_sim=0.4472\n",
      "\n",
      "Analogy (sang - sing + ring):\n",
      "  ring         cosine_sim=0.5789\n",
      "  ring         cosine_sim=0.4186\n",
      "  Ring         cosine_sim=0.3539\n",
      "  ang          cosine_sim=0.3486\n",
      "  Ring         cosine_sim=0.3337\n",
      "\n",
      "Analogy (sing - sang + rang):\n",
      "  rang         cosine_sim=0.6758\n",
      "  sing         cosine_sim=0.6531\n",
      "  Sing         cosine_sim=0.5025\n",
      "  sing         cosine_sim=0.4516\n",
      "  ì‹¨            cosine_sim=0.4257\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    (\"king\", \"man\", \"woman\"),\n",
    "    (\"man\", \"king\", \"queen\"),\n",
    "    (\"walked\", \"walk\", \"jump\"),\n",
    "    (\"go\", \"went\", \"run\"),\n",
    "    (\"sang\", \"sing\", \"ring\"),\n",
    "    (\"sing\", \"sang\", \"rang\"),\n",
    "]\n",
    "\n",
    "for a, b, c in tests:\n",
    "    print(f\"\\nAnalogy ({a} - {b} + {c}):\")\n",
    "    for tok, sim in analogy_a_minus_b_plus_c(tokenizer, embeddings, a, b, c, top_k=5):\n",
    "        print(f\"  {tok:<12} cosine_sim={sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
