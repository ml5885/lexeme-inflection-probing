{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504b2833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelli/Desktop/11424/lexeme-inflection-probing/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import argparse\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"gpt2\": {\n",
    "        \"model_name\": \"gpt2\",\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "    },\n",
    "    \"pythia1.4b\": {\n",
    "        \"model_name\": \"EleutherAI/pythia-1.4b-v0\",\n",
    "        \"tokenizer_name\": \"EleutherAI/pythia-1.4b-v0\",\n",
    "    },\n",
    "    \"gemma2b\": {\n",
    "        \"model_name\": \"google/gemma-2-2b\",\n",
    "        \"tokenizer_name\": \"google/gemma-2-2b\",\n",
    "    },\n",
    "    \"qwen2\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"tokenizer_name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    },\n",
    "    \"bert-base-uncased\": {\n",
    "        \"model_name\": \"bert-base-uncased\",\n",
    "        \"tokenizer_name\": \"bert-base-uncased\",\n",
    "    },\n",
    "    \"bert-large-uncased\": {\n",
    "        \"model_name\": \"bert-large-uncased\",\n",
    "        \"tokenizer_name\": \"bert-large-uncased\",\n",
    "    },\n",
    "    \"distilbert-base-uncased\": {\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"tokenizer_name\": \"distilbert-base-uncased\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_embedding(tokenizer, embeddings, word, method=\"sum\"):\n",
    "    if method == \"tokenize\":\n",
    "        toks = tokenizer.tokenize(word, add_special_tokens=False)\n",
    "        ids = tokenizer.convert_tokens_to_ids(toks)\n",
    "        vecs = embeddings[ids]\n",
    "        return vecs.mean(dim=0)\n",
    "    else: # sum\n",
    "        toks = tokenizer.tokenize(\" \" + word, add_special_tokens=False)\n",
    "        ids = tokenizer.convert_tokens_to_ids(toks)\n",
    "        vecs = embeddings[ids]\n",
    "        return vecs.sum(dim=0)\n",
    "    \n",
    "def get_word_rank(tokenizer, embeddings, query_vec, word, method=\"sum\"):\n",
    "    emb_norm = F.normalize(embeddings, dim=1)\n",
    "    q_norm = F.normalize(query_vec.unsqueeze(0), dim=1)\n",
    "    sims = torch.mm(q_norm, emb_norm.t()).squeeze(0)\n",
    "\n",
    "    if method == \"tokenize\":\n",
    "        toks = tokenizer.tokenize(word, add_special_tokens=False)\n",
    "        ids_for_rank = tokenizer.convert_tokens_to_ids(toks)\n",
    "    else:  # sum\n",
    "        toks = tokenizer.tokenize(\" \" + word, add_special_tokens=False)\n",
    "        ids_for_rank = tokenizer.convert_tokens_to_ids(toks)\n",
    "    \n",
    "    sorted_idxs = torch.argsort(sims, descending=True)\n",
    "    ranks = []\n",
    "    for tid in ids_for_rank:\n",
    "        pos = (sorted_idxs == tid).nonzero(as_tuple=True)[0]\n",
    "        ranks.append(pos.item() + 1)\n",
    "    return sum(ranks) / len(ranks)\n",
    "\n",
    "def find_closest(tokenizer, embeddings, query_vec, top_k=5):\n",
    "    emb_norm = F.normalize(embeddings, dim=1)\n",
    "    q_norm = F.normalize(query_vec.unsqueeze(0), dim=1)\n",
    "    sims = torch.mm(q_norm, emb_norm.t()).squeeze(0)\n",
    "    vals, idxs = torch.topk(sims, k=top_k*10)\n",
    "    results, seen = [], set()\n",
    "    \n",
    "    for score, idx in zip(vals.tolist(), idxs.tolist()):\n",
    "        tok = tokenizer.decode([idx]).strip()\n",
    "        # tok = tokenizer.convert_ids_to_tokens([idx])[0].strip()\n",
    "        if not tok.isalpha() or tok in seen: # don't include byte-level tokens\n",
    "            continue\n",
    "        seen.add(tok)\n",
    "        results.append((tok, score))\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f6fa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analogy (king-man+woman) expecting queen ===\n",
      "\n",
      " method=tokenize: rank of 'queen' = 115\n",
      "   'king'     sim=0.7752\n",
      "   'woman'    sim=0.5815\n",
      "   'ked'      sim=0.5442\n",
      "   'KING'     sim=0.5017\n",
      "   'women'    sim=0.4754\n",
      "\n",
      " method=sum: rank of 'queen' = 2\n",
      "   'king'     sim=0.7758\n",
      "   'queen'    sim=0.7085\n",
      "   'princess' sim=0.6046\n",
      "   'Queen'    sim=0.5964\n",
      "   'kings'    sim=0.5932\n",
      "\n",
      "=== Analogy (man-king+queen) expecting woman ===\n",
      "\n",
      " method=tokenize: rank of 'woman' = 25\n",
      "   'man'      sim=0.5980\n",
      "   'en'       sim=0.4626\n",
      "   'men'      sim=0.4309\n",
      "   'MAN'      sim=0.3857\n",
      "   'que'      sim=0.3844\n",
      "\n",
      " method=sum: rank of 'woman' = 2\n",
      "   'man'      sim=0.6716\n",
      "   'woman'    sim=0.6622\n",
      "   'queen'    sim=0.5638\n",
      "   'lady'     sim=0.4987\n",
      "   'girl'     sim=0.4858\n",
      "\n",
      "=== Analogy (walked-walk+jump) expecting jumped ===\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 8\n",
      "   'jump'     sim=0.8265\n",
      "   'Jump'     sim=0.5245\n",
      "   'ed'       sim=0.5114\n",
      "   'jumps'    sim=0.4896\n",
      "   'jumping'  sim=0.4872\n",
      "\n",
      " method=sum: rank of 'jumped' = 1\n",
      "   'jumped'   sim=0.7761\n",
      "   'jump'     sim=0.7684\n",
      "   'leapt'    sim=0.6663\n",
      "   'jumps'    sim=0.6522\n",
      "   'jumping'  sim=0.6031\n",
      "\n",
      "=== Analogy (go-went+run) expecting ran ===\n",
      "\n",
      " method=tokenize: rank of 'ran' = 24\n",
      "   'run'      sim=0.6597\n",
      "   'go'       sim=0.5318\n",
      "   'Run'      sim=0.4416\n",
      "   'runs'     sim=0.3625\n",
      "   'RUN'      sim=0.3262\n",
      "\n",
      " method=sum: rank of 'ran' = 40\n",
      "   'run'      sim=0.8232\n",
      "   'go'       sim=0.5364\n",
      "   'Run'      sim=0.5148\n",
      "   'runs'     sim=0.4873\n",
      "   'running'  sim=0.4692\n",
      "\n",
      "=== Analogy (sang-sing+ring) expecting rang ===\n",
      "\n",
      " method=tokenize: rank of 'rang' = 2294\n",
      "   'ring'     sim=0.6361\n",
      "   'rings'    sim=0.3957\n",
      "   'ang'      sim=0.3653\n",
      "   'r'        sim=0.2840\n",
      "   'Ring'     sim=0.2836\n",
      "\n",
      " method=sum: rank of 'rang' = 5\n",
      "   'ring'     sim=0.7596\n",
      "   'rings'    sim=0.6185\n",
      "   'Ring'     sim=0.5679\n",
      "   'sang'     sim=0.5266\n",
      "   'rang'     sim=0.5237\n",
      "\n",
      "=== Analogy (sing-sang+rang) expecting ring ===\n",
      "\n",
      " method=tokenize: rank of 'ring' = 41756\n",
      "   'rang'     sim=0.7529\n",
      "   'sing'     sim=0.7290\n",
      "   'RandomRedditor' sim=0.4892\n",
      "   'サーティ'     sim=0.4886\n",
      "   'quickShip' sim=0.4882\n",
      "\n",
      " method=sum: rank of 'ring' = 4\n",
      "   'rang'     sim=0.7107\n",
      "   'sing'     sim=0.5251\n",
      "   'ringing'  sim=0.4633\n",
      "   'ring'     sim=0.4274\n",
      "   'rings'    sim=0.4176\n",
      "\n",
      "=== E('ed') + E('jump') comparison ===\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 1299\n",
      "  top-5: [('jump', '0.8076'), ('ed', '0.7511'), ('ED', '0.5484'), ('jumped', '0.5407'), ('Jump', '0.5406')]\n",
      "  cos_sim=0.4987\n",
      "\n",
      " method=sum: rank of 'jumped' = 6\n",
      "  top-5: [('ed', '0.8046'), ('jump', '0.7395'), ('jumping', '0.5881'), ('jumps', '0.5792'), ('jumped', '0.5597')]\n",
      "  cos_sim=0.5597\n"
     ]
    }
   ],
   "source": [
    "cfg = MODEL_CONFIGS[\"gpt2\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
    "model = AutoModel.from_pretrained(cfg[\"model_name\"])\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "\n",
    "tests = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"man\", \"king\", \"queen\", \"woman\"),\n",
    "    (\"walked\", \"walk\", \"jump\", \"jumped\"),\n",
    "    (\"go\", \"went\", \"run\", \"ran\"),\n",
    "    (\"sang\", \"sing\", \"ring\", \"rang\"),\n",
    "    (\"sing\", \"sang\", \"rang\", \"ring\"),\n",
    "]\n",
    "\n",
    "for a, b, c, d in tests:\n",
    "    print(f\"\\n=== Analogy ({a}-{b}+{c}) expecting {d} ===\")\n",
    "    for method in (\"tokenize\", \"sum\"):\n",
    "        va = get_embedding(tokenizer, embeddings, a, method=method)\n",
    "        vb = get_embedding(tokenizer, embeddings, b, method=method)\n",
    "        vc = get_embedding(tokenizer, embeddings, c, method=method)\n",
    "        query = va - vb + vc\n",
    "\n",
    "        rank = get_word_rank(tokenizer, embeddings, query, d)\n",
    "        print(f\"\\n method={method}: rank of '{d}' = {int(rank)}\")\n",
    "        for tok, sim in find_closest(tokenizer, embeddings, query, top_k=5):\n",
    "            print(f\"   {tok!r:<10} sim={sim:.4f}\")\n",
    "            # print(f\"   {tok!r}  cos_sim={sim:.4f}\")\n",
    "\n",
    "print(\"\\n=== E('ed') + E('jump') comparison ===\")\n",
    "for method in (\"tokenize\", \"sum\"):\n",
    "    v_ed = get_embedding(tokenizer, embeddings, \"ed\",   method=method)\n",
    "    v_jump = get_embedding(tokenizer, embeddings, \"jump\", method=method)\n",
    "    query = v_jump+v_ed\n",
    "\n",
    "    rank = get_word_rank(tokenizer, embeddings, query, \"jumped\", method=method)\n",
    "    print(f\"\\n method={method}: rank of 'jumped' = {int(rank)}\")\n",
    "    print(\"  top-5:\", [(tok, f\"{score:.4f}\") for tok, score in find_closest(tokenizer, embeddings, query, top_k=5)])\n",
    "    \n",
    "    sim = F.cosine_similarity(\n",
    "        query.unsqueeze(0),\n",
    "        get_embedding(tokenizer, embeddings, \"jumped\", method=method).unsqueeze(0),\n",
    "        dim=1\n",
    "    ).item()\n",
    "    print(f\"  cos_sim={sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d68cabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analogy (king-man+woman) expecting queen ===\n",
      "\n",
      " method=tokenize: rank of 'queen' = 2\n",
      "   'king'     sim=0.7370\n",
      "   'queen'    sim=0.6469\n",
      "   'woman'    sim=0.4885\n",
      "   'princess' sim=0.4752\n",
      "   'kings'    sim=0.4659\n",
      "\n",
      " method=sum: rank of 'queen' = 2\n",
      "   'king'     sim=0.7370\n",
      "   'queen'    sim=0.6469\n",
      "   'woman'    sim=0.4885\n",
      "   'princess' sim=0.4752\n",
      "   'kings'    sim=0.4659\n",
      "\n",
      "=== Analogy (man-king+queen) expecting woman ===\n",
      "\n",
      " method=tokenize: rank of 'woman' = 2\n",
      "   'man'      sim=0.7196\n",
      "   'woman'    sim=0.6336\n",
      "   'queen'    sim=0.5380\n",
      "   'girl'     sim=0.5340\n",
      "   'lady'     sim=0.4520\n",
      "\n",
      " method=sum: rank of 'woman' = 2\n",
      "   'man'      sim=0.7196\n",
      "   'woman'    sim=0.6336\n",
      "   'queen'    sim=0.5380\n",
      "   'girl'     sim=0.5340\n",
      "   'lady'     sim=0.4520\n",
      "\n",
      "=== Analogy (walked-walk+jump) expecting jumped ===\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 2\n",
      "   'jump'     sim=0.8086\n",
      "   'jumped'   sim=0.7265\n",
      "   'jumps'    sim=0.6664\n",
      "   'jumping'  sim=0.6360\n",
      "   'leaped'   sim=0.6048\n",
      "\n",
      " method=sum: rank of 'jumped' = 2\n",
      "   'jump'     sim=0.8086\n",
      "   'jumped'   sim=0.7265\n",
      "   'jumps'    sim=0.6664\n",
      "   'jumping'  sim=0.6360\n",
      "   'leaped'   sim=0.6048\n",
      "\n",
      "=== Analogy (go-went+run) expecting ran ===\n",
      "\n",
      " method=tokenize: rank of 'ran' = 7\n",
      "   'run'      sim=0.8394\n",
      "   'go'       sim=0.5375\n",
      "   'running'  sim=0.5096\n",
      "   'runs'     sim=0.4488\n",
      "   'come'     sim=0.3789\n",
      "\n",
      " method=sum: rank of 'ran' = 7\n",
      "   'run'      sim=0.8394\n",
      "   'go'       sim=0.5375\n",
      "   'running'  sim=0.5096\n",
      "   'runs'     sim=0.4488\n",
      "   'come'     sim=0.3789\n",
      "\n",
      "=== Analogy (sang-sing+ring) expecting rang ===\n",
      "\n",
      " method=tokenize: rank of 'rang' = 4\n",
      "   'ring'     sim=0.7252\n",
      "   'rings'    sim=0.5539\n",
      "   'sang'     sim=0.5009\n",
      "   'rang'     sim=0.3941\n",
      "   'jang'     sim=0.3695\n",
      "\n",
      " method=sum: rank of 'rang' = 4\n",
      "   'ring'     sim=0.7252\n",
      "   'rings'    sim=0.5539\n",
      "   'sang'     sim=0.5009\n",
      "   'rang'     sim=0.3941\n",
      "   'jang'     sim=0.3695\n",
      "\n",
      "=== Analogy (sing-sang+rang) expecting ring ===\n",
      "\n",
      " method=tokenize: rank of 'ring' = 2514\n",
      "   'rang'     sim=0.7429\n",
      "   'sing'     sim=0.6292\n",
      "   'ringing'  sim=0.5084\n",
      "   'vibrated' sim=0.4290\n",
      "   'rings'    sim=0.4284\n",
      "\n",
      " method=sum: rank of 'ring' = 2514\n",
      "   'rang'     sim=0.7429\n",
      "   'sing'     sim=0.6292\n",
      "   'ringing'  sim=0.5084\n",
      "   'vibrated' sim=0.4290\n",
      "   'rings'    sim=0.4284\n",
      "\n",
      "=== E('ed') + E('jump') comparison ===\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 4\n",
      "  top-5: [('ed', '0.8025'), ('jump', '0.7955'), ('jumps', '0.6713'), ('jumped', '0.6366'), ('jumping', '0.6150')]\n",
      "  cos_sim=0.6366\n",
      "\n",
      " method=sum: rank of 'jumped' = 4\n",
      "  top-5: [('ed', '0.8025'), ('jump', '0.7955'), ('jumps', '0.6713'), ('jumped', '0.6366'), ('jumping', '0.6150')]\n",
      "  cos_sim=0.6366\n"
     ]
    }
   ],
   "source": [
    "cfg = MODEL_CONFIGS[\"bert-base-uncased\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
    "model = AutoModel.from_pretrained(cfg[\"model_name\"])\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "\n",
    "tests = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"man\", \"king\", \"queen\", \"woman\"),\n",
    "    (\"walked\", \"walk\", \"jump\", \"jumped\"),\n",
    "    (\"go\", \"went\", \"run\", \"ran\"),\n",
    "    (\"sang\", \"sing\", \"ring\", \"rang\"),\n",
    "    (\"sing\", \"sang\", \"rang\", \"ring\"),\n",
    "]\n",
    "\n",
    "for a, b, c, d in tests:\n",
    "    print(f\"\\n=== Analogy ({a}-{b}+{c}) expecting {d} ===\")\n",
    "    for method in (\"tokenize\", \"sum\"):\n",
    "        va = get_embedding(tokenizer, embeddings, a, method=method)\n",
    "        vb = get_embedding(tokenizer, embeddings, b, method=method)\n",
    "        vc = get_embedding(tokenizer, embeddings, c, method=method)\n",
    "        query = va - vb + vc\n",
    "\n",
    "        rank = get_word_rank(tokenizer, embeddings, query, d)\n",
    "        print(f\"\\n method={method}: rank of '{d}' = {int(rank)}\")\n",
    "        for tok, sim in find_closest(tokenizer, embeddings, query, top_k=5):\n",
    "            print(f\"   {tok!r:<10} sim={sim:.4f}\")\n",
    "            # print(f\"   {tok!r}  cos_sim={sim:.4f}\")\n",
    "\n",
    "print(\"\\n=== E('ed') + E('jump') comparison ===\")\n",
    "for method in (\"tokenize\", \"sum\"):\n",
    "    v_ed = get_embedding(tokenizer, embeddings, \"ed\",   method=method)\n",
    "    v_jump = get_embedding(tokenizer, embeddings, \"jump\", method=method)\n",
    "    query = v_jump+v_ed\n",
    "\n",
    "    rank = get_word_rank(tokenizer, embeddings, query, \"jumped\", method=method)\n",
    "    print(f\"\\n method={method}: rank of 'jumped' = {int(rank)}\")\n",
    "    print(\"  top-5:\", [(tok, f\"{score:.4f}\") for tok, score in find_closest(tokenizer, embeddings, query, top_k=5)])\n",
    "    \n",
    "    sim = F.cosine_similarity(\n",
    "        query.unsqueeze(0),\n",
    "        get_embedding(tokenizer, embeddings, \"jumped\", method=method).unsqueeze(0),\n",
    "        dim=1\n",
    "    ).item()\n",
    "    print(f\"  cos_sim={sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8474e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analogy (king-man+woman) expecting queen ===\n",
      "\n",
      " method=tokenize: rank of 'queen' = 6\n",
      "   'king'     sim=0.6939\n",
      "   'KING'     sim=0.4254\n",
      "   'woman'    sim=0.4185\n",
      "   'King'     sim=0.3925\n",
      "   'queen'    sim=0.3881\n",
      "\n",
      " method=sum: rank of 'queen' = 2\n",
      "   'king'     sim=0.6690\n",
      "   'queen'    sim=0.5566\n",
      "   'King'     sim=0.5379\n",
      "   'KING'     sim=0.5199\n",
      "   'kings'    sim=0.5119\n",
      "\n",
      "=== Analogy (man-king+queen) expecting woman ===\n",
      "\n",
      " method=tokenize: rank of 'woman' = 12\n",
      "   'man'      sim=0.6176\n",
      "   'MAN'      sim=0.4735\n",
      "   'queen'    sim=0.4342\n",
      "   'Man'      sim=0.4330\n",
      "   'woman'    sim=0.3886\n",
      "\n",
      " method=sum: rank of 'woman' = 4\n",
      "   'man'      sim=0.7234\n",
      "   'Man'      sim=0.5867\n",
      "   'woman'    sim=0.5149\n",
      "   'MAN'      sim=0.4944\n",
      "   'manifold' sim=0.4252\n",
      "\n",
      "=== Analogy (walked-walk+jump) expecting jumped ===\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 7\n",
      "   'jump'     sim=0.7297\n",
      "   'Jump'     sim=0.5395\n",
      "   'jumping'  sim=0.5046\n",
      "   'jumps'    sim=0.4881\n",
      "   'jumped'   sim=0.4749\n",
      "\n",
      " method=sum: rank of 'jumped' = 2\n",
      "   'jump'     sim=0.8155\n",
      "   'jumped'   sim=0.7497\n",
      "   'jumps'    sim=0.6856\n",
      "   'Jump'     sim=0.6716\n",
      "   'jumping'  sim=0.6367\n",
      "\n",
      "=== Analogy (go-went+run) expecting ran ===\n",
      "\n",
      " method=tokenize: rank of 'ran' = 22\n",
      "   'run'      sim=0.6994\n",
      "   'Run'      sim=0.5133\n",
      "   'go'       sim=0.4472\n",
      "   'runs'     sim=0.4100\n",
      "   'RUN'      sim=0.4040\n",
      "\n",
      " method=sum: rank of 'ran' = 27\n",
      "   'run'      sim=0.8259\n",
      "   'Run'      sim=0.6393\n",
      "   'runs'     sim=0.5343\n",
      "   'RUN'      sim=0.5067\n",
      "   '运行'       sim=0.4647\n",
      "\n",
      "=== Analogy (sang-sing+ring) expecting rang ===\n",
      "\n",
      " method=tokenize: rank of 'rang' = 34\n",
      "   'ring'     sim=0.5789\n",
      "   'Ring'     sim=0.3539\n",
      "   'ang'      sim=0.3486\n",
      "   '环'        sim=0.3169\n",
      "   'rings'    sim=0.3155\n",
      "\n",
      " method=sum: rank of 'rang' = 15\n",
      "   'ring'     sim=0.6934\n",
      "   'Ring'     sim=0.6005\n",
      "   'rings'    sim=0.5945\n",
      "   'Rings'    sim=0.5182\n",
      "   '环'        sim=0.4776\n",
      "\n",
      "=== Analogy (sing-sang+rang) expecting ring ===\n",
      "\n",
      " method=tokenize: rank of 'ring' = 10921\n",
      "   'rang'     sim=0.6758\n",
      "   'sing'     sim=0.6531\n",
      "   'Sing'     sim=0.5025\n",
      "   '싨'        sim=0.4257\n",
      "   '퓭'        sim=0.4253\n",
      "\n",
      " method=sum: rank of 'ring' = 5\n",
      "   'rang'     sim=0.6593\n",
      "   'sing'     sim=0.4605\n",
      "   'Sing'     sim=0.2829\n",
      "   'ring'     sim=0.2807\n",
      "   'range'    sim=0.2501\n",
      "\n",
      "=== E('ed') + E('jump') comparison ===\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 1\n",
      "  top-5: [('ed', '0.7574'), ('jump', '0.6135'), ('ED', '0.4567'), ('Jump', '0.4499'), ('jumping', '0.4159')]\n",
      "  cos_sim=1.0000\n",
      "\n",
      " method=sum: rank of 'jumped' = 9\n",
      "  top-5: [('ed', '0.7214'), ('jump', '0.6801'), ('Jump', '0.5277'), ('Ed', '0.5184'), ('jumps', '0.5134')]\n",
      "  cos_sim=0.4659\n"
     ]
    }
   ],
   "source": [
    "cfg = MODEL_CONFIGS[\"qwen2\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
    "model = AutoModel.from_pretrained(cfg[\"model_name\"])\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "\n",
    "tests = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"man\", \"king\", \"queen\", \"woman\"),\n",
    "    (\"walked\", \"walk\", \"jump\", \"jumped\"),\n",
    "    (\"go\", \"went\", \"run\", \"ran\"),\n",
    "    (\"sang\", \"sing\", \"ring\", \"rang\"),\n",
    "    (\"sing\", \"sang\", \"rang\", \"ring\"),\n",
    "]\n",
    "\n",
    "for a, b, c, d in tests:\n",
    "    print(f\"\\n=== Analogy ({a}-{b}+{c}) expecting {d} ===\")\n",
    "    for method in (\"tokenize\", \"sum\"):\n",
    "        va = get_embedding(tokenizer, embeddings, a, method=method)\n",
    "        vb = get_embedding(tokenizer, embeddings, b, method=method)\n",
    "        vc = get_embedding(tokenizer, embeddings, c, method=method)\n",
    "        query = va - vb + vc\n",
    "\n",
    "        rank = get_word_rank(tokenizer, embeddings, query, d)\n",
    "        print(f\"\\n method={method}: rank of '{d}' = {int(rank)}\")\n",
    "        for tok, sim in find_closest(tokenizer, embeddings, query, top_k=5):\n",
    "            print(f\"   {tok!r:<10} sim={sim:.4f}\")\n",
    "            # print(f\"   {tok!r}  cos_sim={sim:.4f}\")\n",
    "\n",
    "print(\"\\n=== E('ed') + E('jump') comparison ===\")\n",
    "for method in (\"tokenize\", \"sum\"):\n",
    "    v_ed = get_embedding(tokenizer, embeddings, \"ed\",   method=method)\n",
    "    v_jump = get_embedding(tokenizer, embeddings, \"jump\", method=method)\n",
    "    query = v_jump+v_ed\n",
    "\n",
    "    rank = get_word_rank(tokenizer, embeddings, query, \"jumped\", method=method)\n",
    "    print(f\"\\n method={method}: rank of 'jumped' = {int(rank)}\")\n",
    "    print(\"  top-5:\", [(tok, f\"{score:.4f}\") for tok, score in find_closest(tokenizer, embeddings, query, top_k=5)])\n",
    "    \n",
    "    sim = F.cosine_similarity(\n",
    "        query.unsqueeze(0),\n",
    "        get_embedding(tokenizer, embeddings, \"jumped\", method=method).unsqueeze(0),\n",
    "        dim=1\n",
    "    ).item()\n",
    "    print(f\"  cos_sim={sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed8ad9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 38.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analogy (king-man+woman) expecting queen ===\n",
      "\n",
      " method=tokenize: rank of 'queen' = 16\n",
      "   'king'     sim=0.5567\n",
      "   'KING'     sim=0.5456\n",
      "   'woman'    sim=0.5124\n",
      "   'women'    sim=0.4308\n",
      "   'Woman'    sim=0.4152\n",
      "\n",
      " method=sum: rank of 'queen' = 4\n",
      "   'king'     sim=0.6380\n",
      "   'KING'     sim=0.5579\n",
      "   'kings'    sim=0.5487\n",
      "   'queen'    sim=0.5401\n",
      "   'King'     sim=0.4936\n",
      "\n",
      "=== Analogy (man-king+queen) expecting woman ===\n",
      "\n",
      " method=tokenize: rank of 'woman' = 24\n",
      "   'man'      sim=0.5386\n",
      "   'MAN'      sim=0.4883\n",
      "   'queen'    sim=0.4669\n",
      "   'Queen'    sim=0.3955\n",
      "   'Man'      sim=0.3947\n",
      "\n",
      " method=sum: rank of 'woman' = 6\n",
      "   'man'      sim=0.7066\n",
      "   'Man'      sim=0.5206\n",
      "   'MAN'      sim=0.5068\n",
      "   'woman'    sim=0.4132\n",
      "   'hombre'   sim=0.3574\n",
      "\n",
      "=== Analogy (walked-walk+jump) expecting jumped ===\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 5\n",
      "   'jump'     sim=0.7561\n",
      "   'Jump'     sim=0.6496\n",
      "   'jumped'   sim=0.5866\n",
      "   'jumps'    sim=0.5515\n",
      "   'jumping'  sim=0.5470\n",
      "\n",
      " method=sum: rank of 'jumped' = 1\n",
      "   'jumped'   sim=0.8268\n",
      "   'jump'     sim=0.7996\n",
      "   'jumps'    sim=0.7097\n",
      "   'Jump'     sim=0.6977\n",
      "   'jumping'  sim=0.6823\n",
      "\n",
      "=== Analogy (go-went+run) expecting ran ===\n",
      "\n",
      " method=tokenize: rank of 'ran' = 244\n",
      "   'run'      sim=0.7217\n",
      "   'go'       sim=0.5877\n",
      "   'Run'      sim=0.5598\n",
      "   'Go'       sim=0.3714\n",
      "   'to'       sim=0.3675\n",
      "\n",
      " method=sum: rank of 'ran' = 11\n",
      "   'run'      sim=0.8039\n",
      "   'Run'      sim=0.6017\n",
      "   'go'       sim=0.5550\n",
      "   'RUN'      sim=0.4849\n",
      "   'runs'     sim=0.4647\n",
      "\n",
      "=== Analogy (sang-sing+ring) expecting rang ===\n",
      "\n",
      " method=tokenize: rank of 'rang' = 27\n",
      "   'ring'     sim=0.5709\n",
      "   'sang'     sim=0.5544\n",
      "   'RING'     sim=0.4829\n",
      "   'Ring'     sim=0.4795\n",
      "   'rings'    sim=0.4546\n",
      "\n",
      " method=sum: rank of 'rang' = 11\n",
      "   'ring'     sim=0.7248\n",
      "   'Ring'     sim=0.6278\n",
      "   'rings'    sim=0.6105\n",
      "   'RING'     sim=0.5743\n",
      "   'Rings'    sim=0.5299\n",
      "\n",
      "=== Analogy (sing-sang+rang) expecting ring ===\n",
      "\n",
      " method=tokenize: rank of 'ring' = 45\n",
      "   'rang'     sim=0.5714\n",
      "   'sing'     sim=0.5220\n",
      "   'Sing'     sim=0.2758\n",
      "   'ring'     sim=0.2631\n",
      "   'Rang'     sim=0.2509\n",
      "\n",
      " method=sum: rank of 'ring' = 3\n",
      "   'rang'     sim=0.6769\n",
      "   'sing'     sim=0.4566\n",
      "   'ring'     sim=0.3978\n",
      "   'Rang'     sim=0.3726\n",
      "   'ringing'  sim=0.3587\n",
      "\n",
      "=== E('ed') + E('jump') comparison ===\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 1\n",
      "  top-5: [('ed', '0.7276'), ('jump', '0.6527'), ('Jump', '0.5613'), ('ED', '0.5079'), ('jumped', '0.4971')]\n",
      "  cos_sim=1.0000\n",
      "\n",
      " method=sum: rank of 'jumped' = 6\n",
      "  top-5: [('jump', '0.7345'), ('ed', '0.7183'), ('Jump', '0.5717'), ('jumped', '0.5155'), ('jumping', '0.5074')]\n",
      "  cos_sim=0.5155\n"
     ]
    }
   ],
   "source": [
    "cfg = MODEL_CONFIGS[\"gemma2b\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
    "model = AutoModel.from_pretrained(cfg[\"model_name\"])\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "\n",
    "tests = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"man\", \"king\", \"queen\", \"woman\"),\n",
    "    (\"walked\", \"walk\", \"jump\", \"jumped\"),\n",
    "    (\"go\", \"went\", \"run\", \"ran\"),\n",
    "    (\"sang\", \"sing\", \"ring\", \"rang\"),\n",
    "    (\"sing\", \"sang\", \"rang\", \"ring\"),\n",
    "]\n",
    "\n",
    "for a, b, c, d in tests:\n",
    "    print(f\"\\n=== Analogy ({a}-{b}+{c}) expecting {d} ===\")\n",
    "    for method in (\"tokenize\", \"sum\"):\n",
    "        va = get_embedding(tokenizer, embeddings, a, method=method)\n",
    "        vb = get_embedding(tokenizer, embeddings, b, method=method)\n",
    "        vc = get_embedding(tokenizer, embeddings, c, method=method)\n",
    "        query = va - vb + vc\n",
    "\n",
    "        rank = get_word_rank(tokenizer, embeddings, query, d)\n",
    "        print(f\"\\n method={method}: rank of '{d}' = {int(rank)}\")\n",
    "        for tok, sim in find_closest(tokenizer, embeddings, query, top_k=5):\n",
    "            print(f\"   {tok!r:<10} sim={sim:.4f}\")\n",
    "            # print(f\"   {tok!r}  cos_sim={sim:.4f}\")\n",
    "\n",
    "print(\"\\n=== E('ed') + E('jump') comparison ===\")\n",
    "for method in (\"tokenize\", \"sum\"):\n",
    "    v_ed = get_embedding(tokenizer, embeddings, \"ed\",   method=method)\n",
    "    v_jump = get_embedding(tokenizer, embeddings, \"jump\", method=method)\n",
    "    query = v_jump+v_ed\n",
    "\n",
    "    rank = get_word_rank(tokenizer, embeddings, query, \"jumped\", method=method)\n",
    "    print(f\"\\n method={method}: rank of 'jumped' = {int(rank)}\")\n",
    "    print(\"  top-5:\", [(tok, f\"{score:.4f}\") for tok, score in find_closest(tokenizer, embeddings, query, top_k=5)])\n",
    "    \n",
    "    sim = F.cosine_similarity(\n",
    "        query.unsqueeze(0),\n",
    "        get_embedding(tokenizer, embeddings, \"jumped\", method=method).unsqueeze(0),\n",
    "        dim=1\n",
    "    ).item()\n",
    "    print(f\"  cos_sim={sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
