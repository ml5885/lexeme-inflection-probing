{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504b2833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelli/Desktop/11424/lexeme-inflection-probing/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import argparse\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"gpt2\": {\n",
    "        \"model_name\": \"gpt2\",\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "    },\n",
    "    \"pythia1.4b\": {\n",
    "        \"model_name\": \"EleutherAI/pythia-1.4b-v0\",\n",
    "        \"tokenizer_name\": \"EleutherAI/pythia-1.4b-v0\",\n",
    "    },\n",
    "    \"gemma2b\": {\n",
    "        \"model_name\": \"google/gemma-2-2b\",\n",
    "        \"tokenizer_name\": \"google/gemma-2-2b\",\n",
    "    },\n",
    "    \"qwen2\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"tokenizer_name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    },\n",
    "    \"bert-base-uncased\": {\n",
    "        \"model_name\": \"bert-base-uncased\",\n",
    "        \"tokenizer_name\": \"bert-base-uncased\",\n",
    "    },\n",
    "    \"bert-large-uncased\": {\n",
    "        \"model_name\": \"bert-large-uncased\",\n",
    "        \"tokenizer_name\": \"bert-large-uncased\",\n",
    "    },\n",
    "    \"distilbert-base-uncased\": {\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"tokenizer_name\": \"distilbert-base-uncased\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_embedding(tokenizer, embeddings, word, method=\"sum\"):\n",
    "    if method == \"tokenize\":\n",
    "        toks = tokenizer.tokenize(word, add_special_tokens=False)\n",
    "        print(\"tokenize: \", toks)\n",
    "        ids = tokenizer.convert_tokens_to_ids(toks)\n",
    "        vecs = embeddings[ids]\n",
    "        return vecs.mean(dim=0)\n",
    "    else: # sum\n",
    "        toks = tokenizer.tokenize(\" \" + word, add_special_tokens=False)\n",
    "        print(\"sum: \", toks)\n",
    "        ids = tokenizer.convert_tokens_to_ids(toks)\n",
    "        vecs = embeddings[ids]\n",
    "        return vecs.sum(dim=0)\n",
    "    \n",
    "def get_word_rank(tokenizer, embeddings, query_vec, word, method=\"sum\"):\n",
    "    emb_norm = F.normalize(embeddings, dim=1)\n",
    "    q_norm = F.normalize(query_vec.unsqueeze(0), dim=1)\n",
    "    sims = torch.mm(q_norm, emb_norm.t()).squeeze(0)\n",
    "\n",
    "    if method == \"tokenize\":\n",
    "        toks = tokenizer.tokenize(word, add_special_tokens=False)\n",
    "        ids_for_rank = tokenizer.convert_tokens_to_ids(toks)\n",
    "    else:  # sum\n",
    "        toks = tokenizer.tokenize(\" \" + word, add_special_tokens=False)\n",
    "        ids_for_rank = tokenizer.convert_tokens_to_ids(toks)\n",
    "    \n",
    "    sorted_idxs = torch.argsort(sims, descending=True)\n",
    "    ranks = []\n",
    "    for tid in ids_for_rank:\n",
    "        pos = (sorted_idxs == tid).nonzero(as_tuple=True)[0]\n",
    "        ranks.append(pos.item() + 1)\n",
    "    return sum(ranks) / len(ranks)\n",
    "\n",
    "def find_closest(tokenizer, embeddings, query_vec, top_k=5):\n",
    "    emb_norm = F.normalize(embeddings, dim=1)\n",
    "    q_norm = F.normalize(query_vec.unsqueeze(0), dim=1)\n",
    "    sims = torch.mm(q_norm, emb_norm.t()).squeeze(0)\n",
    "    vals, idxs = torch.topk(sims, k=top_k*10)\n",
    "    results, seen = [], set()\n",
    "    \n",
    "    for score, idx in zip(vals.tolist(), idxs.tolist()):\n",
    "        tok = tokenizer.decode([idx]).strip()\n",
    "        # tok = tokenizer.convert_ids_to_tokens([idx])[0].strip()\n",
    "        if not tok.isalpha() or tok in seen: # don't include byte-level tokens\n",
    "            continue\n",
    "        seen.add(tok)\n",
    "        results.append((tok, score))\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d68cabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analogy (king-man+woman) expecting queen ===\n",
      "tokenize:  ['king']\n",
      "tokenize:  ['man']\n",
      "tokenize:  ['woman']\n",
      "\n",
      " method=tokenize: rank of 'queen' = 2\n",
      "   'king'     sim=0.7370\n",
      "   'queen'    sim=0.6469\n",
      "   'woman'    sim=0.4885\n",
      "   'princess' sim=0.4752\n",
      "   'kings'    sim=0.4659\n",
      "sum:  ['king']\n",
      "sum:  ['man']\n",
      "sum:  ['woman']\n",
      "\n",
      " method=sum: rank of 'queen' = 2\n",
      "   'king'     sim=0.7370\n",
      "   'queen'    sim=0.6469\n",
      "   'woman'    sim=0.4885\n",
      "   'princess' sim=0.4752\n",
      "   'kings'    sim=0.4659\n",
      "\n",
      "=== Analogy (man-king+queen) expecting woman ===\n",
      "tokenize:  ['man']\n",
      "tokenize:  ['king']\n",
      "tokenize:  ['queen']\n",
      "\n",
      " method=tokenize: rank of 'woman' = 2\n",
      "   'man'      sim=0.7196\n",
      "   'woman'    sim=0.6336\n",
      "   'queen'    sim=0.5380\n",
      "   'girl'     sim=0.5340\n",
      "   'lady'     sim=0.4520\n",
      "sum:  ['man']\n",
      "sum:  ['king']\n",
      "sum:  ['queen']\n",
      "\n",
      " method=sum: rank of 'woman' = 2\n",
      "   'man'      sim=0.7196\n",
      "   'woman'    sim=0.6336\n",
      "   'queen'    sim=0.5380\n",
      "   'girl'     sim=0.5340\n",
      "   'lady'     sim=0.4520\n",
      "\n",
      "=== Analogy (walked-walk+jump) expecting jumped ===\n",
      "tokenize:  ['walked']\n",
      "tokenize:  ['walk']\n",
      "tokenize:  ['jump']\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 2\n",
      "   'jump'     sim=0.8086\n",
      "   'jumped'   sim=0.7265\n",
      "   'jumps'    sim=0.6664\n",
      "   'jumping'  sim=0.6360\n",
      "   'leaped'   sim=0.6048\n",
      "sum:  ['walked']\n",
      "sum:  ['walk']\n",
      "sum:  ['jump']\n",
      "\n",
      " method=sum: rank of 'jumped' = 2\n",
      "   'jump'     sim=0.8086\n",
      "   'jumped'   sim=0.7265\n",
      "   'jumps'    sim=0.6664\n",
      "   'jumping'  sim=0.6360\n",
      "   'leaped'   sim=0.6048\n",
      "\n",
      "=== Analogy (go-went+run) expecting ran ===\n",
      "tokenize:  ['go']\n",
      "tokenize:  ['went']\n",
      "tokenize:  ['run']\n",
      "\n",
      " method=tokenize: rank of 'ran' = 7\n",
      "   'run'      sim=0.8394\n",
      "   'go'       sim=0.5375\n",
      "   'running'  sim=0.5096\n",
      "   'runs'     sim=0.4488\n",
      "   'come'     sim=0.3789\n",
      "sum:  ['go']\n",
      "sum:  ['went']\n",
      "sum:  ['run']\n",
      "\n",
      " method=sum: rank of 'ran' = 7\n",
      "   'run'      sim=0.8394\n",
      "   'go'       sim=0.5375\n",
      "   'running'  sim=0.5096\n",
      "   'runs'     sim=0.4488\n",
      "   'come'     sim=0.3789\n",
      "\n",
      "=== Analogy (sang-sing+ring) expecting rang ===\n",
      "tokenize:  ['sang']\n",
      "tokenize:  ['sing']\n",
      "tokenize:  ['ring']\n",
      "\n",
      " method=tokenize: rank of 'rang' = 4\n",
      "   'ring'     sim=0.7252\n",
      "   'rings'    sim=0.5539\n",
      "   'sang'     sim=0.5009\n",
      "   'rang'     sim=0.3941\n",
      "   'jang'     sim=0.3695\n",
      "sum:  ['sang']\n",
      "sum:  ['sing']\n",
      "sum:  ['ring']\n",
      "\n",
      " method=sum: rank of 'rang' = 4\n",
      "   'ring'     sim=0.7252\n",
      "   'rings'    sim=0.5539\n",
      "   'sang'     sim=0.5009\n",
      "   'rang'     sim=0.3941\n",
      "   'jang'     sim=0.3695\n",
      "\n",
      "=== Analogy (sing-sang+rang) expecting ring ===\n",
      "tokenize:  ['sing']\n",
      "tokenize:  ['sang']\n",
      "tokenize:  ['rang']\n",
      "\n",
      " method=tokenize: rank of 'ring' = 2514\n",
      "   'rang'     sim=0.7429\n",
      "   'sing'     sim=0.6292\n",
      "   'ringing'  sim=0.5084\n",
      "   'vibrated' sim=0.4290\n",
      "   'rings'    sim=0.4284\n",
      "sum:  ['sing']\n",
      "sum:  ['sang']\n",
      "sum:  ['rang']\n",
      "\n",
      " method=sum: rank of 'ring' = 2514\n",
      "   'rang'     sim=0.7429\n",
      "   'sing'     sim=0.6292\n",
      "   'ringing'  sim=0.5084\n",
      "   'vibrated' sim=0.4290\n",
      "   'rings'    sim=0.4284\n",
      "\n",
      "=== E('ed') + E('jump') comparison ===\n",
      "tokenize:  ['ed']\n",
      "tokenize:  ['jump']\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 4\n",
      "  top-5: [('ed', '0.8025'), ('jump', '0.7955'), ('jumps', '0.6713'), ('jumped', '0.6366'), ('jumping', '0.6150')]\n",
      "tokenize:  ['jumped']\n",
      "  cos_sim=0.6366\n",
      "sum:  ['ed']\n",
      "sum:  ['jump']\n",
      "\n",
      " method=sum: rank of 'jumped' = 4\n",
      "  top-5: [('ed', '0.8025'), ('jump', '0.7955'), ('jumps', '0.6713'), ('jumped', '0.6366'), ('jumping', '0.6150')]\n",
      "sum:  ['jumped']\n",
      "  cos_sim=0.6366\n"
     ]
    }
   ],
   "source": [
    "cfg = MODEL_CONFIGS[\"bert-base-uncased\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
    "model = AutoModel.from_pretrained(cfg[\"model_name\"])\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "\n",
    "tests = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"man\", \"king\", \"queen\", \"woman\"),\n",
    "    (\"walked\", \"walk\", \"jump\", \"jumped\"),\n",
    "    (\"go\", \"went\", \"run\", \"ran\"),\n",
    "    (\"sang\", \"sing\", \"ring\", \"rang\"),\n",
    "    (\"sing\", \"sang\", \"rang\", \"ring\"),\n",
    "]\n",
    "\n",
    "for a, b, c, d in tests:\n",
    "    print(f\"\\n=== Analogy ({a}-{b}+{c}) expecting {d} ===\")\n",
    "    for method in (\"tokenize\", \"sum\"):\n",
    "        va = get_embedding(tokenizer, embeddings, a, method=method)\n",
    "        vb = get_embedding(tokenizer, embeddings, b, method=method)\n",
    "        vc = get_embedding(tokenizer, embeddings, c, method=method)\n",
    "        query = va - vb + vc\n",
    "\n",
    "        rank = get_word_rank(tokenizer, embeddings, query, d)\n",
    "        print(f\"\\n method={method}: rank of '{d}' = {int(rank)}\")\n",
    "        for tok, sim in find_closest(tokenizer, embeddings, query, top_k=5):\n",
    "            print(f\"   {tok!r:<10} sim={sim:.4f}\")\n",
    "            # print(f\"   {tok!r}  cos_sim={sim:.4f}\")\n",
    "\n",
    "print(\"\\n=== E('ed') + E('jump') comparison ===\")\n",
    "for method in (\"tokenize\", \"sum\"):\n",
    "    v_ed = get_embedding(tokenizer, embeddings, \"ed\",   method=method)\n",
    "    v_jump = get_embedding(tokenizer, embeddings, \"jump\", method=method)\n",
    "    query = v_jump+v_ed\n",
    "\n",
    "    rank = get_word_rank(tokenizer, embeddings, query, \"jumped\", method=method)\n",
    "    print(f\"\\n method={method}: rank of 'jumped' = {int(rank)}\")\n",
    "    print(\"  top-5:\", [(tok, f\"{score:.4f}\") for tok, score in find_closest(tokenizer, embeddings, query, top_k=5)])\n",
    "    \n",
    "    sim = F.cosine_similarity(\n",
    "        query.unsqueeze(0),\n",
    "        get_embedding(tokenizer, embeddings, \"jumped\", method=method).unsqueeze(0),\n",
    "        dim=1\n",
    "    ).item()\n",
    "    print(f\"  cos_sim={sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f6fa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analogy (king-man+woman) expecting queen ===\n",
      "tokenize:  ['king']\n",
      "tokenize:  ['man']\n",
      "tokenize:  ['woman']\n",
      "\n",
      " method=tokenize: rank of 'queen' = 115\n",
      "   'king'     sim=0.7752\n",
      "   'woman'    sim=0.5815\n",
      "   'ked'      sim=0.5442\n",
      "   'KING'     sim=0.5017\n",
      "   'women'    sim=0.4754\n",
      "sum:  ['Ġking']\n",
      "sum:  ['Ġman']\n",
      "sum:  ['Ġwoman']\n",
      "\n",
      " method=sum: rank of 'queen' = 2\n",
      "   'king'     sim=0.7758\n",
      "   'queen'    sim=0.7085\n",
      "   'princess' sim=0.6046\n",
      "   'Queen'    sim=0.5964\n",
      "   'kings'    sim=0.5932\n",
      "\n",
      "=== Analogy (man-king+queen) expecting woman ===\n",
      "tokenize:  ['man']\n",
      "tokenize:  ['king']\n",
      "tokenize:  ['que', 'en']\n",
      "\n",
      " method=tokenize: rank of 'woman' = 25\n",
      "   'man'      sim=0.5980\n",
      "   'en'       sim=0.4626\n",
      "   'men'      sim=0.4309\n",
      "   'MAN'      sim=0.3857\n",
      "   'que'      sim=0.3844\n",
      "sum:  ['Ġman']\n",
      "sum:  ['Ġking']\n",
      "sum:  ['Ġqueen']\n",
      "\n",
      " method=sum: rank of 'woman' = 2\n",
      "   'man'      sim=0.6716\n",
      "   'woman'    sim=0.6622\n",
      "   'queen'    sim=0.5638\n",
      "   'lady'     sim=0.4987\n",
      "   'girl'     sim=0.4858\n",
      "\n",
      "=== Analogy (walked-walk+jump) expecting jumped ===\n",
      "tokenize:  ['walk', 'ed']\n",
      "tokenize:  ['walk']\n",
      "tokenize:  ['jump']\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 8\n",
      "   'jump'     sim=0.8265\n",
      "   'Jump'     sim=0.5245\n",
      "   'ed'       sim=0.5114\n",
      "   'jumps'    sim=0.4896\n",
      "   'jumping'  sim=0.4872\n",
      "sum:  ['Ġwalked']\n",
      "sum:  ['Ġwalk']\n",
      "sum:  ['Ġjump']\n",
      "\n",
      " method=sum: rank of 'jumped' = 1\n",
      "   'jumped'   sim=0.7761\n",
      "   'jump'     sim=0.7684\n",
      "   'leapt'    sim=0.6663\n",
      "   'jumps'    sim=0.6522\n",
      "   'jumping'  sim=0.6031\n",
      "\n",
      "=== Analogy (go-went+run) expecting ran ===\n",
      "tokenize:  ['go']\n",
      "tokenize:  ['went']\n",
      "tokenize:  ['run']\n",
      "\n",
      " method=tokenize: rank of 'ran' = 24\n",
      "   'run'      sim=0.6597\n",
      "   'go'       sim=0.5318\n",
      "   'Run'      sim=0.4416\n",
      "   'runs'     sim=0.3625\n",
      "   'RUN'      sim=0.3262\n",
      "sum:  ['Ġgo']\n",
      "sum:  ['Ġwent']\n",
      "sum:  ['Ġrun']\n",
      "\n",
      " method=sum: rank of 'ran' = 40\n",
      "   'run'      sim=0.8232\n",
      "   'go'       sim=0.5364\n",
      "   'Run'      sim=0.5148\n",
      "   'runs'     sim=0.4873\n",
      "   'running'  sim=0.4692\n",
      "\n",
      "=== Analogy (sang-sing+ring) expecting rang ===\n",
      "tokenize:  ['s', 'ang']\n",
      "tokenize:  ['sing']\n",
      "tokenize:  ['ring']\n",
      "\n",
      " method=tokenize: rank of 'rang' = 2294\n",
      "   'ring'     sim=0.6361\n",
      "   'rings'    sim=0.3957\n",
      "   'ang'      sim=0.3653\n",
      "   'r'        sim=0.2840\n",
      "   'Ring'     sim=0.2836\n",
      "sum:  ['Ġsang']\n",
      "sum:  ['Ġsing']\n",
      "sum:  ['Ġring']\n",
      "\n",
      " method=sum: rank of 'rang' = 5\n",
      "   'ring'     sim=0.7596\n",
      "   'rings'    sim=0.6185\n",
      "   'Ring'     sim=0.5679\n",
      "   'sang'     sim=0.5266\n",
      "   'rang'     sim=0.5237\n",
      "\n",
      "=== Analogy (sing-sang+rang) expecting ring ===\n",
      "tokenize:  ['sing']\n",
      "tokenize:  ['s', 'ang']\n",
      "tokenize:  ['rang']\n",
      "\n",
      " method=tokenize: rank of 'ring' = 41756\n",
      "   'rang'     sim=0.7529\n",
      "   'sing'     sim=0.7290\n",
      "   'RandomRedditor' sim=0.4892\n",
      "   'サーティ'     sim=0.4886\n",
      "   'quickShip' sim=0.4882\n",
      "sum:  ['Ġsing']\n",
      "sum:  ['Ġsang']\n",
      "sum:  ['Ġrang']\n",
      "\n",
      " method=sum: rank of 'ring' = 4\n",
      "   'rang'     sim=0.7107\n",
      "   'sing'     sim=0.5251\n",
      "   'ringing'  sim=0.4633\n",
      "   'ring'     sim=0.4274\n",
      "   'rings'    sim=0.4176\n",
      "\n",
      "=== E('ed') + E('jump') comparison ===\n",
      "tokenize:  ['ed']\n",
      "tokenize:  ['jump']\n",
      "\n",
      " method=tokenize: rank of 'jumped' = 1299\n",
      "  top-5: [('jump', '0.8076'), ('ed', '0.7511'), ('ED', '0.5484'), ('jumped', '0.5407'), ('Jump', '0.5406')]\n",
      "tokenize:  ['j', 'umped']\n",
      "  cos_sim=0.4987\n",
      "sum:  ['Ġed']\n",
      "sum:  ['Ġjump']\n",
      "\n",
      " method=sum: rank of 'jumped' = 6\n",
      "  top-5: [('ed', '0.8046'), ('jump', '0.7395'), ('jumping', '0.5881'), ('jumps', '0.5792'), ('jumped', '0.5597')]\n",
      "sum:  ['Ġjumped']\n",
      "  cos_sim=0.5597\n"
     ]
    }
   ],
   "source": [
    "cfg = MODEL_CONFIGS[\"gpt2\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
    "model = AutoModel.from_pretrained(cfg[\"model_name\"])\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "\n",
    "tests = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"man\", \"king\", \"queen\", \"woman\"),\n",
    "    (\"walked\", \"walk\", \"jump\", \"jumped\"),\n",
    "    (\"go\", \"went\", \"run\", \"ran\"),\n",
    "    (\"sang\", \"sing\", \"ring\", \"rang\"),\n",
    "    (\"sing\", \"sang\", \"rang\", \"ring\"),\n",
    "]\n",
    "\n",
    "for a, b, c, d in tests:\n",
    "    print(f\"\\n=== Analogy ({a}-{b}+{c}) expecting {d} ===\")\n",
    "    for method in (\"tokenize\", \"sum\"):\n",
    "        va = get_embedding(tokenizer, embeddings, a, method=method)\n",
    "        vb = get_embedding(tokenizer, embeddings, b, method=method)\n",
    "        vc = get_embedding(tokenizer, embeddings, c, method=method)\n",
    "        query = va - vb + vc\n",
    "\n",
    "        rank = get_word_rank(tokenizer, embeddings, query, d)\n",
    "        print(f\"\\n method={method}: rank of '{d}' = {int(rank)}\")\n",
    "        for tok, sim in find_closest(tokenizer, embeddings, query, top_k=5):\n",
    "            print(f\"   {tok!r:<10} sim={sim:.4f}\")\n",
    "            # print(f\"   {tok!r}  cos_sim={sim:.4f}\")\n",
    "\n",
    "print(\"\\n=== E('ed') + E('jump') comparison ===\")\n",
    "for method in (\"tokenize\", \"sum\"):\n",
    "    v_ed = get_embedding(tokenizer, embeddings, \"ed\",   method=method)\n",
    "    v_jump = get_embedding(tokenizer, embeddings, \"jump\", method=method)\n",
    "    query = v_jump+v_ed\n",
    "\n",
    "    rank = get_word_rank(tokenizer, embeddings, query, \"jumped\", method=method)\n",
    "    print(f\"\\n method={method}: rank of 'jumped' = {int(rank)}\")\n",
    "    print(\"  top-5:\", [(tok, f\"{score:.4f}\") for tok, score in find_closest(tokenizer, embeddings, query, top_k=5)])\n",
    "    \n",
    "    sim = F.cosine_similarity(\n",
    "        query.unsqueeze(0),\n",
    "        get_embedding(tokenizer, embeddings, \"jumped\", method=method).unsqueeze(0),\n",
    "        dim=1\n",
    "    ).item()\n",
    "    print(f\"  cos_sim={sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = MODEL_CONFIGS[\"qwen2\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
    "model = AutoModel.from_pretrained(cfg[\"model_name\"])\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "\n",
    "tests = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"man\", \"king\", \"queen\", \"woman\"),\n",
    "    (\"walked\", \"walk\", \"jump\", \"jumped\"),\n",
    "    (\"go\", \"went\", \"run\", \"ran\"),\n",
    "    (\"sang\", \"sing\", \"ring\", \"rang\"),\n",
    "    (\"sing\", \"sang\", \"rang\", \"ring\"),\n",
    "]\n",
    "\n",
    "for a, b, c, d in tests:\n",
    "    print(f\"\\n=== Analogy ({a}-{b}+{c}) expecting {d} ===\")\n",
    "    for method in (\"tokenize\", \"sum\"):\n",
    "        va = get_embedding(tokenizer, embeddings, a, method=method)\n",
    "        vb = get_embedding(tokenizer, embeddings, b, method=method)\n",
    "        vc = get_embedding(tokenizer, embeddings, c, method=method)\n",
    "        query = va - vb + vc\n",
    "\n",
    "        rank = get_word_rank(tokenizer, embeddings, query, d)\n",
    "        print(f\"\\n method={method}: rank of '{d}' = {int(rank)}\")\n",
    "        for tok, sim in find_closest(tokenizer, embeddings, query, top_k=5):\n",
    "            print(f\"   {tok!r:<10} sim={sim:.4f}\")\n",
    "            # print(f\"   {tok!r}  cos_sim={sim:.4f}\")\n",
    "\n",
    "print(\"\\n=== E('ed') + E('jump') comparison ===\")\n",
    "for method in (\"tokenize\", \"sum\"):\n",
    "    v_ed = get_embedding(tokenizer, embeddings, \"ed\",   method=method)\n",
    "    v_jump = get_embedding(tokenizer, embeddings, \"jump\", method=method)\n",
    "    query = v_jump+v_ed\n",
    "\n",
    "    rank = get_word_rank(tokenizer, embeddings, query, \"jumped\", method=method)\n",
    "    print(f\"\\n method={method}: rank of 'jumped' = {int(rank)}\")\n",
    "    print(\"  top-5:\", [(tok, f\"{score:.4f}\") for tok, score in find_closest(tokenizer, embeddings, query, top_k=5)])\n",
    "    \n",
    "    sim = F.cosine_similarity(\n",
    "        query.unsqueeze(0),\n",
    "        get_embedding(tokenizer, embeddings, \"jumped\", method=method).unsqueeze(0),\n",
    "        dim=1\n",
    "    ).item()\n",
    "    print(f\"  cos_sim={sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ad9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = MODEL_CONFIGS[\"gemma2b\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
    "model = AutoModel.from_pretrained(cfg[\"model_name\"])\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "\n",
    "tests = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"man\", \"king\", \"queen\", \"woman\"),\n",
    "    (\"walked\", \"walk\", \"jump\", \"jumped\"),\n",
    "    (\"go\", \"went\", \"run\", \"ran\"),\n",
    "    (\"sang\", \"sing\", \"ring\", \"rang\"),\n",
    "    (\"sing\", \"sang\", \"rang\", \"ring\"),\n",
    "]\n",
    "\n",
    "for a, b, c, d in tests:\n",
    "    print(f\"\\n=== Analogy ({a}-{b}+{c}) expecting {d} ===\")\n",
    "    for method in (\"tokenize\", \"sum\"):\n",
    "        va = get_embedding(tokenizer, embeddings, a, method=method)\n",
    "        vb = get_embedding(tokenizer, embeddings, b, method=method)\n",
    "        vc = get_embedding(tokenizer, embeddings, c, method=method)\n",
    "        query = va - vb + vc\n",
    "\n",
    "        rank = get_word_rank(tokenizer, embeddings, query, d)\n",
    "        print(f\"\\n method={method}: rank of '{d}' = {int(rank)}\")\n",
    "        for tok, sim in find_closest(tokenizer, embeddings, query, top_k=5):\n",
    "            print(f\"   {tok!r:<10} sim={sim:.4f}\")\n",
    "            # print(f\"   {tok!r}  cos_sim={sim:.4f}\")\n",
    "\n",
    "print(\"\\n=== E('ed') + E('jump') comparison ===\")\n",
    "for method in (\"tokenize\", \"sum\"):\n",
    "    v_ed = get_embedding(tokenizer, embeddings, \"ed\",   method=method)\n",
    "    v_jump = get_embedding(tokenizer, embeddings, \"jump\", method=method)\n",
    "    query = v_jump+v_ed\n",
    "\n",
    "    rank = get_word_rank(tokenizer, embeddings, query, \"jumped\", method=method)\n",
    "    print(f\"\\n method={method}: rank of 'jumped' = {int(rank)}\")\n",
    "    print(\"  top-5:\", [(tok, f\"{score:.4f}\") for tok, score in find_closest(tokenizer, embeddings, query, top_k=5)])\n",
    "    \n",
    "    sim = F.cosine_similarity(\n",
    "        query.unsqueeze(0),\n",
    "        get_embedding(tokenizer, embeddings, \"jumped\", method=method).unsqueeze(0),\n",
    "        dim=1\n",
    "    ).item()\n",
    "    print(f\"  cos_sim={sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
