{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UniMorph Dataset Generation for Probing\n",
    "\n",
    "This notebook extracts morphological paradigms from UniMorph (English) and generates controlled template sentences. The final dataset contains metadata (lexeme, category, inflection label, dimension, etc.) so we can later filter or pair examples for our probing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached UniMorph data from data/eng.unimorph.tsv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "um_url = \"https://raw.githubusercontent.com/unimorph/eng/master/eng\"\n",
    "\n",
    "# Check if UniMorph English data exists locally\n",
    "file_path = \"../data/eng.unimorph.tsv\"\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"Using cached UniMorph data from {file_path}\")\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Download UniMorph English data from GitHub\n",
    "    print(f\"Downloading UniMorph data from {um_url}...\")\n",
    "    response = requests.get(um_url)\n",
    "    \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "        \n",
    "    num_lines = len(response.text.strip().split('\\n'))\n",
    "    \n",
    "    print(f\"Downloaded UniMorph data with {num_lines} lines and saved to {file_path}\")\n",
    "    \n",
    "um_lines = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        um_lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of the dataset:\n",
      "microtome\tmicrotomes\tN;PL\n",
      "microtome\tmicrotomes\tV;PRS;3;SG\n",
      "microtome\tmicrotoming\tV;V.PTCP;PRS\n",
      "microtome\tmicrotomed\tV;PST\n",
      "microtome\tmicrotomed\tV;V.PTCP;PST\n",
      "\n",
      "Last 5 lines of the dataset:\n",
      "myriadaire\tmyriadaire\tN;SG\n",
      "dibridgehead\tdibridgehead\tN;SG\n",
      "Chicagoese\tChicagoese\tN;SG\n",
      "Druzer\tDruzer\tN;SG\n",
      "electrosensible\telectrosensible\tN;SG\n",
      "\n",
      "Number of unique lexemes: 399574\n",
      "\n",
      "Distribution by category:\n",
      "Noun: 388561 entries (59.6%)\n",
      "Verb: 127513 entries (19.5%)\n",
      "Adjective: 136398 entries (20.9%)\n"
     ]
    }
   ],
   "source": [
    "# Check that everything looks good\n",
    "print(f\"First 5 lines of the dataset:\")\n",
    "for line in um_lines[:5]:\n",
    "    print(line)\n",
    "print()\n",
    "\n",
    "print(f\"Last 5 lines of the dataset:\")\n",
    "for line in um_lines[-5:]:\n",
    "    print(line)\n",
    "print()\n",
    "\n",
    "# Number of unique lexemes in data\n",
    "lexemes = set(line.split('\\t')[0] for line in um_lines)\n",
    "print(f\"Number of unique lexemes: {len(lexemes)}\")\n",
    "\n",
    "# Print some basic statistics\n",
    "categories = {}\n",
    "for line in um_lines:\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) == 3:\n",
    "        features = parts[2]\n",
    "        if \"V;\" in features:\n",
    "            cat = \"Verb\"\n",
    "        elif \"N;\" in features:\n",
    "            cat = \"Noun\"\n",
    "        elif \"ADJ\" in features:\n",
    "            cat = \"Adjective\"\n",
    "        else:\n",
    "            cat = \"Other\"\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"\\nDistribution by category:\")\n",
    "for cat, count in categories.items():\n",
    "    print(f\"{cat}: {count} entries ({count/len(um_lines)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build the paradim table. For each line (format: lexeme \\t inflected_form \\t features), we map lexemes to a dict of inflection labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 399574 lexemes after filtering (filter_multi = False )\n"
     ]
    }
   ],
   "source": [
    "def get_category_and_label_english(features):\n",
    "    feats = features.split(\";\")\n",
    "    if \"V\" in feats:\n",
    "        category = \"Verb\"\n",
    "        if \"PRES\" in feats:\n",
    "            if \"3\" in feats and \"SG\" in feats:\n",
    "                label = \"3rd_pers\"\n",
    "            else:\n",
    "                label = \"base\"\n",
    "        elif \"PST\" in feats:\n",
    "            label = \"past\"\n",
    "        elif \"ING\" in feats or \"PTCP\" in feats:\n",
    "            label = \"present_participle\"\n",
    "        else:\n",
    "            label = \"base\"\n",
    "        dimension = \"Tense/Aspect\"\n",
    "    elif \"N\" in feats:\n",
    "        category = \"Noun\"\n",
    "        label = \"plural\" if \"PL\" in feats else \"singular\"\n",
    "        dimension = \"Number\"\n",
    "    elif \"ADJ\" in feats:\n",
    "        category = \"Adjective\"\n",
    "        if \"COMP\" in feats:\n",
    "            label = \"comparative\"\n",
    "        elif \"SUP\" in feats:\n",
    "            label = \"superlative\"\n",
    "        else:\n",
    "            label = \"positive\"\n",
    "        dimension = \"Degree\"\n",
    "    else:\n",
    "        category, label, dimension = None, None, None\n",
    "    return category, label, dimension\n",
    "\n",
    "paradigms = {}\n",
    "for line in um_lines:\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "    if len(parts) != 3:\n",
    "        continue\n",
    "    lemma, form, feats = parts\n",
    "    cat, lab, dim = get_category_and_label_english(feats)\n",
    "    if cat is None:\n",
    "        continue\n",
    "    if lemma not in paradigms:\n",
    "        paradigms[lemma] = {\"category\": cat, \"dimension\": dim, \"forms\": {}}\n",
    "    if lab not in paradigms[lemma][\"forms\"]:\n",
    "        paradigms[lemma][\"forms\"][lab] = []\n",
    "    if form not in paradigms[lemma][\"forms\"][lab]:\n",
    "        paradigms[lemma][\"forms\"][lab].append(form)\n",
    "\n",
    "# Flag to filter lexemes with only one inflection\n",
    "filter_multi = False  # Set to True to restrict to lexemes with >1 inflection\n",
    "\n",
    "if filter_multi:\n",
    "    paradigms = {lemma: info for lemma, info in paradigms.items() if len(info[\"forms\"]) > 1}\n",
    "\n",
    "print(\"Using\", len(paradigms), \"lexemes after filtering (filter_multi =\", filter_multi, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexeme: telesurgery\n",
      "Category: Noun\n",
      "Dimension: Number\n",
      "singular: ['telesurgery']\n",
      "\n",
      "Lexeme: overbidder\n",
      "Category: Noun\n",
      "Dimension: Number\n",
      "plural: ['overbidders']\n",
      "singular: ['overbidder']\n",
      "\n",
      "Lexeme: floud\n",
      "Category: Noun\n",
      "Dimension: Number\n",
      "plural: ['flouds']\n",
      "singular: ['floud']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# print some randomly sampled examples\n",
    "for lexeme, data in random.sample(list(paradigms.items()), 3):\n",
    "    print(f\"\\nLexeme: {lexeme}\")\n",
    "    print(f\"Category: {data['category']}\")\n",
    "    print(f\"Dimension: {data['dimension']}\")\n",
    "    for label, form in data[\"forms\"].items():\n",
    "        print(f\"{label}: {form}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each lexeme in the paradigm table, generate sentences from fixed templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 261330 controlled template sentences.\n",
      "Controlled sentences saved to data/controlled_sentences.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the complete set of desired labels for each category.\n",
    "desired_labels = {\n",
    "    \"Verb\": [\"base\", \"3rd_pers\", \"past\", \"present_participle\"],\n",
    "    \"Noun\": [\"singular\", \"plural\"],\n",
    "    \"Adjective\": [\"positive\", \"comparative\", \"superlative\"]\n",
    "}\n",
    "\n",
    "# Define extremely generic (vague) templates.\n",
    "templates = {\n",
    "    \"Verb\": {\n",
    "        \"base\": [\n",
    "            \"I {}.\"\n",
    "        ],\n",
    "        \"3rd_pers\": [\n",
    "            \"He {}.\"\n",
    "        ],\n",
    "        \"past\": [\n",
    "            \"I {} in the past.\"\n",
    "        ],\n",
    "        \"present_participle\": [\n",
    "            \"I am {}.\"\n",
    "        ]\n",
    "    },\n",
    "    \"Noun\": {\n",
    "        \"singular\": [\n",
    "            \"It is {}.\"\n",
    "        ],\n",
    "        \"plural\": [\n",
    "            \"They are {}.\"\n",
    "        ]\n",
    "    },\n",
    "    \"Adjective\": {\n",
    "        \"positive\": [\n",
    "            \"It is {}.\"\n",
    "        ],\n",
    "        \"comparative\": [\n",
    "            \"It is {} than before.\"\n",
    "        ],\n",
    "        \"superlative\": [\n",
    "            \"It is the {} one.\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def select_form_english(category, label, forms_list, lemma):\n",
    "    # If the unimorph data provides the form for the label, choose it via heuristics;\n",
    "    # otherwise, fall back to the lemma.\n",
    "    if forms_list:\n",
    "        if category == \"Verb\":\n",
    "            if label == \"base\":\n",
    "                for f in forms_list:\n",
    "                    if not (f.endswith(\"s\") or f.endswith(\"ing\") or (f.endswith(\"ed\") and f != forms_list[0])):\n",
    "                        return f\n",
    "                return forms_list[0]\n",
    "            elif label == \"3rd_pers\":\n",
    "                for f in forms_list:\n",
    "                    if f.endswith(\"s\"):\n",
    "                        return f\n",
    "                return forms_list[0]\n",
    "            elif label == \"past\":\n",
    "                for f in forms_list:\n",
    "                    if f.endswith(\"ate\"):\n",
    "                        return f\n",
    "                for f in forms_list:\n",
    "                    if not f.endswith(\"ing\") and not f.endswith(\"en\"):\n",
    "                        return f\n",
    "                return forms_list[0]\n",
    "            elif label == \"present_participle\":\n",
    "                for f in forms_list:\n",
    "                    if f.endswith(\"ing\"):\n",
    "                        return f\n",
    "                return forms_list[0]\n",
    "        elif category == \"Noun\":\n",
    "            if label == \"singular\":\n",
    "                for f in forms_list:\n",
    "                    if not f.endswith(\"s\"):\n",
    "                        return f\n",
    "                return forms_list[0]\n",
    "            elif label == \"plural\":\n",
    "                for f in forms_list:\n",
    "                    if f.endswith(\"s\"):\n",
    "                        return f\n",
    "                return forms_list[0]\n",
    "        elif category == \"Adjective\":\n",
    "            return forms_list[0]\n",
    "    # Fallback: use the lemma.\n",
    "    return lemma\n",
    "\n",
    "controlled_examples = []\n",
    "for lemma, info in paradigms.items():\n",
    "    cat = info[\"category\"]\n",
    "    dim = info[\"dimension\"]\n",
    "    # For each desired label in the category, either use the available form or fall back to the lemma.\n",
    "    for lab in desired_labels.get(cat, []):\n",
    "        if lab in info[\"forms\"]:\n",
    "            forms_list = info[\"forms\"][lab]\n",
    "        else:\n",
    "            forms_list = []\n",
    "        selected_form = select_form_english(cat, lab, forms_list, lemma)\n",
    "        for temp in templates[cat][lab]:\n",
    "            sentence = temp.format(selected_form)\n",
    "            tokens = sentence.split()\n",
    "            try:\n",
    "                target_index = tokens.index(selected_form)\n",
    "            except ValueError:\n",
    "                target_index = -1\n",
    "            if target_index < 0:\n",
    "                continue\n",
    "            controlled_examples.append({\n",
    "                \"Sentence\": sentence,\n",
    "                \"Target Index\": target_index,\n",
    "                \"Lemma\": lemma,\n",
    "                \"Category\": cat,\n",
    "                \"Inflection Label\": lab,\n",
    "                \"Word Form\": selected_form,\n",
    "                \"Dimension\": dim,\n",
    "                \"Source Type\": \"Template\"\n",
    "            })\n",
    "\n",
    "df_controlled = pd.DataFrame(controlled_examples)\n",
    "print(\"Generated\", len(df_controlled), \"controlled template sentences.\")\n",
    "df_controlled.head(10)\n",
    "\n",
    "# save to CSV\n",
    "output_file = \"../data/controlled_sentences.csv\"\n",
    "df_controlled.to_csv(output_file, index=False)\n",
    "print(f\"Controlled sentences saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Target Index</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Category</th>\n",
       "      <th>Inflection Label</th>\n",
       "      <th>Word Form</th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Source Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203761</th>\n",
       "      <td>It is seedborne than before.</td>\n",
       "      <td>2</td>\n",
       "      <td>seedborne</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>comparative</td>\n",
       "      <td>seedborne</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51509</th>\n",
       "      <td>It is right-thinking than before.</td>\n",
       "      <td>2</td>\n",
       "      <td>right-thinking</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>comparative</td>\n",
       "      <td>right-thinking</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186863</th>\n",
       "      <td>It is biophysiochemical than before.</td>\n",
       "      <td>2</td>\n",
       "      <td>biophysiochemical</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>comparative</td>\n",
       "      <td>biophysiochemical</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18075</th>\n",
       "      <td>It is quartzy than before.</td>\n",
       "      <td>2</td>\n",
       "      <td>quartzy</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>comparative</td>\n",
       "      <td>quartzy</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257898</th>\n",
       "      <td>I unstiffen in the past.</td>\n",
       "      <td>1</td>\n",
       "      <td>unstiffen</td>\n",
       "      <td>Verb</td>\n",
       "      <td>past</td>\n",
       "      <td>unstiffen</td>\n",
       "      <td>Tense/Aspect</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103227</th>\n",
       "      <td>It is inconstruable than before.</td>\n",
       "      <td>2</td>\n",
       "      <td>inconstruable</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>comparative</td>\n",
       "      <td>inconstruable</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190648</th>\n",
       "      <td>It is the nonesoteric one.</td>\n",
       "      <td>3</td>\n",
       "      <td>nonesoteric</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>superlative</td>\n",
       "      <td>nonesoteric</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181568</th>\n",
       "      <td>It is the paparazzied one.</td>\n",
       "      <td>3</td>\n",
       "      <td>paparazzied</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>superlative</td>\n",
       "      <td>paparazzied</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199876</th>\n",
       "      <td>It is the physiopathogenic one.</td>\n",
       "      <td>3</td>\n",
       "      <td>physiopathogenic</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>superlative</td>\n",
       "      <td>physiopathogenic</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219229</th>\n",
       "      <td>It is Chardinian than before.</td>\n",
       "      <td>2</td>\n",
       "      <td>Chardinian</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>comparative</td>\n",
       "      <td>Chardinian</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Template</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Sentence  Target Index              Lemma  \\\n",
       "203761          It is seedborne than before.             2          seedborne   \n",
       "51509      It is right-thinking than before.             2     right-thinking   \n",
       "186863  It is biophysiochemical than before.             2  biophysiochemical   \n",
       "18075             It is quartzy than before.             2            quartzy   \n",
       "257898              I unstiffen in the past.             1          unstiffen   \n",
       "103227      It is inconstruable than before.             2      inconstruable   \n",
       "190648            It is the nonesoteric one.             3        nonesoteric   \n",
       "181568            It is the paparazzied one.             3        paparazzied   \n",
       "199876       It is the physiopathogenic one.             3   physiopathogenic   \n",
       "219229         It is Chardinian than before.             2         Chardinian   \n",
       "\n",
       "         Category Inflection Label          Word Form     Dimension  \\\n",
       "203761  Adjective      comparative          seedborne        Degree   \n",
       "51509   Adjective      comparative     right-thinking        Degree   \n",
       "186863  Adjective      comparative  biophysiochemical        Degree   \n",
       "18075   Adjective      comparative            quartzy        Degree   \n",
       "257898       Verb             past          unstiffen  Tense/Aspect   \n",
       "103227  Adjective      comparative      inconstruable        Degree   \n",
       "190648  Adjective      superlative        nonesoteric        Degree   \n",
       "181568  Adjective      superlative        paparazzied        Degree   \n",
       "199876  Adjective      superlative   physiopathogenic        Degree   \n",
       "219229  Adjective      comparative         Chardinian        Degree   \n",
       "\n",
       "       Source Type  \n",
       "203761    Template  \n",
       "51509     Template  \n",
       "186863    Template  \n",
       "18075     Template  \n",
       "257898    Template  \n",
       "103227    Template  \n",
       "190648    Template  \n",
       "181568    Template  \n",
       "199876    Template  \n",
       "219229    Template  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot check the data\n",
    "df_controlled.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Sentence\": \"string\",\n",
      "  \"Target Index\": \"integer\",\n",
      "  \"Lemma\": \"string\",\n",
      "  \"Category\": \"string\",\n",
      "  \"Inflection Label\": \"string\",\n",
      "  \"Word Form\": \"string\",\n",
      "  \"Dimension\": \"string\",\n",
      "  \"Source Type\": \"string\"\n",
      "}\n",
      "{\n",
      "  \"Sentence\": \"I ate in the past.\",\n",
      "  \"Target Index\": 1,\n",
      "  \"Lemma\": \"eat\",\n",
      "  \"Category\": \"Verb\",\n",
      "  \"Inflection Label\": \"past\",\n",
      "  \"Word Form\": \"ate\",\n",
      "  \"Dimension\": \"Tense/Aspect\",\n",
      "  \"Source Type\": \"Template\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# print out the schema\n",
    "schema = {\n",
    "    \"Sentence\": \"string\",\n",
    "    \"Target Index\": \"integer\",\n",
    "    \"Lemma\": \"string\",\n",
    "    \"Category\": \"string\",\n",
    "    \"Inflection Label\": \"string\",\n",
    "    \"Word Form\": \"string\",\n",
    "    \"Dimension\": \"string\",\n",
    "    \"Source Type\": \"string\"\n",
    "}\n",
    "print(json.dumps(schema, indent=2))\n",
    "\n",
    "# example entry\n",
    "example_entry = df_controlled.iloc[0].to_dict()\n",
    "print(json.dumps(example_entry, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the sentences we generate don't look very natural, probably because we use very naive templates. I've thought of a couple of ways to improves this (e.g. use a language model to generate sentences), but I think a better approach will be to grab context sentences of wordforms and their inflections from the WikiText dataset. The next few code cells do exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Frequency threshold: candidate words must appear at least this many times.\n",
    "min_occurrences = 10\n",
    "# Maximum sentences to store per candidate to avoid memory bloat.\n",
    "max_sentences_per_candidate = 50\n",
    "\n",
    "# Compile candidate entries from our paradigms (convert to lowercase for matching)\n",
    "candidates = []\n",
    "for lemma, info in paradigms.items():\n",
    "    cat = info[\"category\"]\n",
    "    for lab, forms_list in info[\"forms\"].items():\n",
    "        for form in forms_list:\n",
    "            candidates.append({\n",
    "                \"lemma\": lemma,\n",
    "                \"category\": cat,\n",
    "                \"inflection_label\": lab,\n",
    "                \"word_form\": form.lower()\n",
    "            })\n",
    "\n",
    "# Build a dictionary mapping candidate word (lower-case) to a tuple: (count, list_of_sentences)\n",
    "candidate_sentences = { cand[\"word_form\"]: [0, []] for cand in candidates }\n",
    "candidate_set = set(candidate_sentences.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing WikiText entries: 1801350it [02:03, 14594.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55826 candidate word forms with at least 10 occurrences in WikiText.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wikitext = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\", streaming=True)\n",
    "\n",
    "# Process each WikiText entry with a progress bar.\n",
    "for entry in tqdm(wikitext, desc=\"Processing WikiText entries\"):\n",
    "    text = entry[\"text\"]\n",
    "    if not text.strip():\n",
    "        continue\n",
    "    # Naively split text into sentences using period.\n",
    "    sentences = text.split(\".\")\n",
    "    for sentence in sentences:\n",
    "        sent = sentence.strip()\n",
    "        if not sent:\n",
    "            continue\n",
    "        # Tokenize sentence into words (convert tokens to lower case for matching)\n",
    "        tokens = re.findall(r\"\\w+\", sent.lower())\n",
    "        token_set = set(tokens)\n",
    "        common = candidate_set.intersection(token_set)\n",
    "        if common:\n",
    "            for cand in common:\n",
    "                count, sents_list = candidate_sentences[cand]\n",
    "                candidate_sentences[cand][0] = count + 1  # increment count\n",
    "                # Only store up to max_sentences_per_candidate sentences.\n",
    "                if len(sents_list) < max_sentences_per_candidate:\n",
    "                    sents_list.append(sent)\n",
    "\n",
    "# Filter out candidate word forms with fewer occurrences than the threshold.\n",
    "filtered_candidates = {cand: (count, sents) for cand, (count, sents) in candidate_sentences.items() if count >= min_occurrences}\n",
    "print(\"Found\", len(filtered_candidates), \"candidate word forms with at least\", min_occurrences, \"occurrences in WikiText.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing, using 30811 candidate word forms (each with 50 sentences).\n",
      "Wikitext sentences dataset has 2276982 rows.\n",
      "Probing input dataset saved as 'wikitext_sentences.csv'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set the desired number of sentences per candidate.\n",
    "target_examples = 50\n",
    "\n",
    "# Filter candidates based on the threshold (min_occurrences) from before.\n",
    "# 'filtered_candidates' was defined earlier as:\n",
    "#   { cand: (count, sents) for cand, (count, sents) in candidate_sentences.items() if count >= min_occurrences }\n",
    "# Here, we rebalance so that each candidate with at least target_examples sentences provides exactly target_examples samples.\n",
    "balanced_candidates = {\n",
    "    cand: (count, random.sample(sents, target_examples))\n",
    "    for cand, (count, sents) in filtered_candidates.items()\n",
    "    if len(sents) >= target_examples\n",
    "}\n",
    "print(\"After balancing, using\", len(balanced_candidates), \"candidate word forms (each with\", target_examples, \"sentences).\")\n",
    "\n",
    "# Build a mapping from candidate word (lowercase) to its metadata entries.\n",
    "metadata_map = {}\n",
    "for cand in candidates:\n",
    "    key = cand[\"word_form\"]\n",
    "    if key not in metadata_map:\n",
    "        metadata_map[key] = []\n",
    "    metadata_map[key].append(cand)\n",
    "\n",
    "# Assemble rows for the balanced natural context dataset.\n",
    "rows = []\n",
    "for cand, (count, sents) in balanced_candidates.items():\n",
    "    # For each candidate word form, add each corresponding metadata entry.\n",
    "    for meta in metadata_map.get(cand, []):\n",
    "        for sentence in sents:\n",
    "            # Determine the index of the candidate word (using case-insensitive matching).\n",
    "            orig_tokens = sentence.split()\n",
    "            idx = -1\n",
    "            for i, token in enumerate(orig_tokens):\n",
    "                if token.lower() == cand:\n",
    "                    idx = i\n",
    "                    break\n",
    "            if idx >= 0:\n",
    "                rows.append({\n",
    "                    \"Sentence\": sentence,\n",
    "                    \"Target Index\": idx,\n",
    "                    \"Lemma\": meta[\"lemma\"],\n",
    "                    \"Category\": meta[\"category\"],\n",
    "                    \"Inflection Label\": meta[\"inflection_label\"],\n",
    "                    \"Word Form\": cand,\n",
    "                    \"Source Type\": \"NaturalWikiText\"\n",
    "                })\n",
    "\n",
    "df_wikitext_sentences = pd.DataFrame(rows)\n",
    "print(\"Wikitext sentences dataset has\", len(df_wikitext_sentences), \"rows.\")\n",
    "df_wikitext_sentences.head(10)\n",
    "\n",
    "df_wikitext_sentences.to_csv(\"../data/wikitext_sentences.csv\", index=False)\n",
    "\n",
    "print(\"Probing input dataset saved as 'wikitext_sentences.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset has 2538312 rows. Saved as 'combined_sentences.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Combine and shufle these datasets\n",
    "combined_df = pd.concat([df_controlled, df_wikitext_sentences], ignore_index=True)\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "combined_df.to_csv(\"../data/combined_sentences.csv\", index=False)\n",
    "print(\"Combined dataset has\", len(combined_df), \"rows. Saved as 'combined_sentences.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu",
   "language": "python",
   "name": "cmu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
